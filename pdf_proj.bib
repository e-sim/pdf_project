@article{Goodman2018,
abstract = {Extracting semi-structured text from scientific writing in PDF files is a difficult task that researchers have faced for decades. In the 1990s, this task was largely a computer vision and OCR problem, as PDF files were often the result of scanning printed documents. Today, PDFs have standardized digital typesetting without the need for OCR, but extraction of semi-structured text from these documents remains a nontrivial task. In this paper, we present a system for the reanalysis of glyph-level PDF-extracted text that performs block detection, respacing, and tabular data analysis for the purposes of linguistic data mining. We further present our reanalyzed output format, which attempts to eliminate the extreme verbosity of XML output while leaving important positional information available for downstream processes.},
author = {Goodman, Michael Wayne and Georgi, Ryan and Xia, Fei},
file = {:C$\backslash$:/Users/Erica/Documents/CLMS/OCR{\_}proj/947.pdf:pdf},
journal = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
keywords = {Corpus Creation,Interlinear Glossed Text (IGT),Low Resource Languages},
pages = {723--727},
title = {{PDF-to-Text Reanalysis for Linguistic Data Mining}},
year = {2018}
}
@misc{Jiang2010,
abstract = {The invention provides a PDF text extraction method in combination with OCR technology, which belongs to the technical field of pattern recognition. The method comprises the following steps: (1) extracting PDF data; (2) confirming a character content in combination with the OCR technology; (3) processing a second code of the character; and (4) educing the second code of the character processed in the step (3) according to a position, font and word size of the character. In the method, the OCR technology is combined during the confirmation process of a computer internal code of the character so as to effectively improve the accuracy of PDF text extraction and solve the problem of the incapability of extracting the character content in a PDF document.},
author = {Jiang, Shisheng and Liu, Qiang},
file = {:C$\backslash$:/Users/Erica/Downloads/CN101782896B.pdf:pdf},
publisher = {China National Intellectual Property Administration},
title = {{PDF text extraction method in combination with OCR technology}},
year = {2010}
}
@book{Moran2018,
address = {Berlin},
author = {Moran, Steven and Cysouw, Michael},
doi = {10.5281/zenodo.1296780},
file = {:C$\backslash$:/Users/Erica/Downloads/176-3-1135-2-10-20180702.pdf:pdf},
isbn = {9783961100903},
publisher = {Language Science Press},
title = {{The Unicode Cookbook for Linguists orthography profiles}},
year = {2018}
}
@article{Bird2002,
author = {Bird, Steven and Dale, Robert and Dorr, Bonnie J and Gibson, Bryan and Joseph, Mark T and Kan, Min-yen and Lee, Dongwon and Powley, Brett and Radev, Dragomir R and Tan, Yee Fan},
file = {:C$\backslash$:/Users/Erica/Documents/CLMS/OCR{\_}proj/readings/67771{\_}00004320{\_}01{\_}lrec08.pdf:pdf},
title = {{The ACL Anthology Reference Corpus : A Reference Dataset for Bibliographic Research in Computational Linguistics}},
year = {2002}
}
@article{Tiedemann2014,
annote = {Good overview of problems in PDF extraction
Section 2.6 involves language detection

Google Compact Language Detector library!!!!!

their implementation is open source},
author = {Tiedemann, J{\"{o}}rg},
file = {:C$\backslash$:/Users/Erica/Documents/CLMS/OCR{\_}proj/readings/1717450.pdf:pdf},
journal = {Lecture Notes in Computer Science},
number = {1},
pages = {192--112},
title = {{Improved Text Extraction from PDF Documents for Large-Scale Natural Language Processing}},
volume = {8403},
year = {2014}
}
@inproceedings{Bast2017,
abstract = {Extracting the body text from a PDF document is an important but surprisingly difficult task. The reason is that PDF is a layout-based format which specifies the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). There is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality bench- mark of principally arbitrary size from parallel TeX and PDF data. We construct such a benchmark of 12,098 scientific articles from arXiv.org and make it publicly available. We establish a set of crite- ria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which significantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to finally make text extraction from PDF a "solved problem."},
address = {Toronto, Ontario, Canada},
archivePrefix = {arXiv},
arxivId = {arXiv:1709.01073v2},
author = {Bast, Hannah and Korzen, Claudius},
booktitle = {Proceedings of Joint Conference On Digital Libraries},
doi = {10.1145/nnnnnnn.nnnnnnn},
eprint = {arXiv:1709.01073v2},
file = {:C$\backslash$:/Users/Erica/Documents/CLMS/OCR{\_}proj/pdf{\_}ocr.pdf:pdf},
isbn = {1558606041},
issn = {16130073},
keywords = {Benchmark,Evaluation,PDF,Text Extraction,ocr-project},
mendeley-tags = {ocr-project},
pmid = {397311},
title = {{A Benchmark and Evaluation for Text Extraction from PDF}},
year = {2017}
}
@article{Ramakrishnan2012,
author = {Ramakrishnan, Cartic and Patnia, Abishek and Hovy, Eduard and Burns, Gully APC},
file = {:C$\backslash$:/Users/Erica/Documents/CLMS/OCR{\_}proj/readings/1717448.pdf:pdf},
journal = {Source code for biology and medicine},
number = {1},
pages = {7},
title = {{Layout-aware text extraction from full-text PDF of scientific articles}},
volume = {7},
year = {2012}
}
@article{Id2015,
author = {Karad, A. R.},
file = {:C$\backslash$:/Users/Erica/Documents/CLMS/OCR{\_}proj/readings/1717477.pdf:pdf},
journal = {International Journal of Applied Engineering Research},
number = {3},
pages = {7721--7726},
title = {{Rule Based Chunk Extraction from PDF Documents Using Regular Expressions and Natural Language Processing}},
volume = {10},
year = {2015}
}
@article{Hassan2006,
abstract = {In recent years, PDF has become the de-facto standard for the exchange of print-oriented documents on the Web. This includes many business documents such as financial reports, newsletters and patent applications, and there are many commercial applications that require data to be extracted from these documents and processed by computer systems. A number of products currently exist on the market that navigate, extract and transform data from HTML pages; a process known as wrapping. One such methodology is Lixto, a product of research at our institute. However, none of these products are currently able to work with PDF files. We are investigating this possibility as part of the NEX-TWRAP project. This paper describes our work in progress, and details some of the low-level page segmentation techniques that we have investigated},
annote = {mostly PDF to HTML
Lixto software
whitespace, layout},
author = {Hassan, T. and Baumgartner, R.},
doi = {10.1109/cimca.2005.1631436},
file = {:C$\backslash$:/Users/Erica/Documents/CLMS/OCR{\_}proj/readings/01631436.pdf:pdf},
isbn = {0769525040},
pages = {2--6},
title = {{Intelligent Text Extraction from PDF Documents}},
year = {2006}
}
@article{Vol2018,
author = {Vol, Mark and Krutsko, Andrew and Stefanovitch, Nicolas and Postanogov, Denis},
doi = {10.1109/DAS.2018.64},
file = {:C$\backslash$:/Users/Erica/Documents/CLMS/OCR{\_}proj/readings/1717476.pdf:pdf},
isbn = {9781538633465},
journal = {2018 13th IAPR International Workshop on Document Analysis Systems (DAS)},
keywords = {-document handling,academic,corrupted font recovery,document restoration,embedded fonts,little attention in the,optical character recogni-,pdf documents,this topic received very,tion},
pages = {121--126},
publisher = {IEEE},
title = {{Automatic Recovery of Corrupted Font Encoding in PDF Documents Using CNN-Based Symbol Recognition with Language Model}},
year = {2018}
}
@article{Lee2003,
author = {Lee, Chung-hong and Yang, Hsin-chang},
doi = {10.1023/A},
file = {:C$\backslash$:/Users/Erica/Downloads/A{\_}Multilingual{\_}Text{\_}Mining{\_}Approach{\_}Based{\_}on{\_}Self-.pdf:pdf},
journal = {Applied Intelligence},
keywords = {information,knowledge extraction,multilingual,multilingual text mining,pdf,self-organizing maps,text mining,web mining},
mendeley-tags = {pdf,text mining,multilingual},
number = {May},
pages = {295--310},
title = {{A Multilingual Text Mining Approach Based on Self-Organizing Maps A Multilingual Text Mining Approach Based on Self-Organizing Maps}},
volume = {18},
year = {2003}
}
@phdthesis{Berg2011,
annote = {p1-26 good for background (also check references from section 2 for more possible papers)

uses PDFBox},
author = {Berg, {\O}yvind Raddum},
file = {:C$\backslash$:/Users/Erica/Documents/CLMS/OCR{\_}proj/readings/Berg.pdf:pdf},
pages = {103},
school = {University of Oslo},
title = {{High precision text extraction from PDF documents}},
year = {2011}
}
